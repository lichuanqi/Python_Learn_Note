import gradio as gr
import random
import time

from util import wenxin_to_chatbot, chatbot_to_wenxin
from api import ACCESS_TIME, ACCESS_TOKEN
from api import update_access_token, chat_by_token

from config import cfg


llm_model_dict = cfg.readValue("basic", "llm_model_dict")
knowadge_index_dict = cfg.readValue("basic", "knowadge_index_dict")
embedding_model_dict = cfg.readValue("basic", "embedding_model_dict")

llm_choices = list(llm_model_dict.keys())
knowadge_choices = list(knowadge_index_dict.keys())
embedding_choices=list(embedding_model_dict.keys())


def request_api_base(message, chatbot):
    """æ–‡æœ¬ç”Ÿæˆä¸»å‡½æ•°, ä¸»è¦ç”¨æˆ·è°ƒè¯•æ¯æ¬¡éƒ½è¿”å›å›ºå®šçš„å­—ç¬¦ä¸²
    
    Params
        message: è¾“å…¥
        history: 

    Return
        response: å¯¹åº”äºæ‰€æœ‰ç”¨æˆ·å’Œæœºå™¨äººå“åº”çš„å­—ç¬¦ä¸²å…ƒç»„åˆ—è¡¨ã€‚
                  è¿™å°†åœ¨ Gradio æ¼”ç¤ºä¸­å‘ˆç°ä¸ºè¾“å‡ºã€‚
        history: æ‰€æœ‰ç”¨æˆ·å’Œæœºå™¨äººå“åº”çš„ä»¤ç‰Œè¡¨ç¤ºã€‚
                 åœ¨æœ‰çŠ¶æ€çš„ Gradio æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨å‡½æ•°ç»“æŸæ—¶è¿”å›æ›´æ–°çš„çŠ¶æ€ã€‚
    """
    history = chatbot_to_wenxin(chatbot)
    history.append({"role": "user", "content": message})
    response = "ä½ å¥½ä½ å¥½"
    history.append({"role": "assistant", "content": response})
    
    messages = wenxin_to_chatbot(history)

    return "", messages


def request_api_wenxin(message, chatbot):
    """æ–‡æœ¬ç”Ÿæˆä¸»å‡½æ•°, è°ƒç”¨æ–‡å¿ƒä¸€è¨€æ¥å£"""
    update_access_token(ACCESS_TIME, ACCESS_TOKEN)

    history = chatbot_to_wenxin(chatbot)
    history.append({"role": "user","content": message})
    reponse = chat_by_token(ACCESS_TOKEN, history, stream=False)

    # å­—ç¬¦ä¸²ç»“æœ
    result = reponse["result"]
    # tokensæ•°
    prompt_tokens, completion_tokens = reponse["usage"]["prompt_tokens"], reponse["usage"]["completion_tokens"]

    history.append({"role": "assistant","content": result})
    messages = wenxin_to_chatbot(history)

    return "", messages


def onButtomSubmit(use_knowadge, 
                   vector_search_top_k, 
                   history_len, 
                   temperature, 
                   top_p, 
                   message, 
                   chatbot):
    """"æäº¤æŒ‰é’®æ§½å‡½æ•°

    Params
        use_knowadge        : æ˜¯å¦ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“
        vector_search_top_k : å‘é‡æ•°æ®åº“æ£€ç´¢TOP K
        history_len,        : æœ€å¤§å¯¹è¯è½®æ•°
        temperature, 
        top_p, 
        message
        chatbot
    
    Return
        message
        chatbot
    """
    # æ‹¼æ¥æœ¬åœ°çŸ¥è¯†åº“å†…å®¹
    if use_knowadge == "æ˜¯":
        print('ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢')

    # è°ƒç”¨APIæ¥å£
    history = chatbot_to_wenxin(chatbot)
    history.append({"role": "user","content": message})

    update_access_token(ACCESS_TIME, ACCESS_TOKEN)
    reponse = chat_by_token(ACCESS_TOKEN, history, stream=False)
    result = reponse["result"]
    
    # ç»Ÿè®¡tokensæ•°
    prompt_tokens, completion_tokens = reponse["usage"]["prompt_tokens"], reponse["usage"]["completion_tokens"]
    history.append({"role": "assistant","content": result})
    messages = wenxin_to_chatbot(history)

    return "", messages


def onButtomClear():
    print('å·²æ¸…ç©ºå†å²æ¶ˆæ¯')
    return [], None


def main():
    with gr.Blocks(title="LLM") as demo:
                #    theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
                    ## ğŸš€LLMåŠ©æ‰‹
                    ä½¿ç”¨æ–‡å¿ƒä¸€è¨€å•†ç”¨APIå®ç°çš„å¤§è¯­è¨€æ¨¡å‹åŠ©æ‰‹        
                    """)
        history = []
        state = gr.State([])

        with gr.Row():
            with gr.Column(scale=1):
                with gr.Accordion("æœ¬åœ°çŸ¥è¯†åº“"):
                    use_knowadge = gr.Radio(
                        label="use knowadge",
                        choices=knowadge_choices,
                        value="ä¸ä½¿ç”¨")
                    embedding_model = gr.Dropdown(
                        label="Embedding model",
                        choices=embedding_choices,
                        value=embedding_choices[0])
                    vector_search_top_k = gr.Slider(
                        1,
                        10,
                        value=6,
                        step=1,
                        label="vector search top k",
                        interactive=True)
                    # upload_file = gr.File(
                    #     label='å¢åŠ çŸ¥è¯†åº“æ–‡ä»¶',
                    #     file_types=['.txt', '.md'])
                    
                with gr.Accordion("æ¨¡å‹å‚æ•°é…ç½®"):
                    large_language_model = gr.Dropdown(
                        choices=llm_choices,
                        label="large language model",
                        value=llm_choices[0])
                    history_len = gr.Slider(
                        0,
                        5,
                        value=3,
                        step=1,
                        label="history len",
                        interactive=True)
                    temperature = gr.Slider(
                        0,
                        1,
                        value=0.95,
                        step=0.01,
                        label="temperature",
                        interactive=True)
                    top_p = gr.Slider(
                        0,
                        1,
                        value=0.7,
                        step=0.1,
                        label="top_p",
                        interactive=True)

            with gr.Column(scale=4):
                chatbot = gr.Chatbot(height=450)
                msg = gr.Textbox(
                    label="Chat Message Box",
                    placeholder="è¯·è¾“å…¥è¦æé—®çš„é—®é¢˜",
                    show_label=False,
                    container=False)
                
                gr.Examples(
                    examples=[
                        "å†™ä¸€ä¸ªå¤§å­¦ç”Ÿå¿ƒç†æƒ…æ™¯å‰§å¤§èµ›çš„å‚èµ›å‰§æœ¬ï¼Œæè¿°å¤§å­¦ç”Ÿæ´»ä¸­å°‘å¹´Aä»è¿·èŒ«åˆ°æ‰¾åˆ°å¥‹æ–—æ–¹å‘çš„æ•…äº‹",
                        "å¸®æˆ‘è®¾è®¡ä¸€ä¸ªå¯ä»¥æ‰“å‡ºsinå‡½æ•°çš„ä»£ç ",
                        "æŸ¥ä¸€ä¸ªçŸ¥è¯†ï¼š"],
                    inputs=msg,
                    outputs=None,
                    cache_examples=False,
                    label="å¿«é€Ÿæé—®")

                with gr.Column():
                    with gr.Row():
                        submit = gr.Button("Submit")
                        stop = gr.Button("Stop")
                        clear = gr.Button("Clear")
        
        # è¾“å…¥æ¡†å›è½¦æ§½å‡½æ•°
        msg_event = msg.submit(
            fn=onButtomSubmit,
            inputs=[use_knowadge, vector_search_top_k, 
                    history_len, temperature, top_p, msg, chatbot],
            outputs=[msg, chatbot],
            queue=True)
        # æäº¤æŒ‰é’®æ§½å‡½æ•°
        submit_event = submit.click(
            fn=onButtomSubmit,
            inputs=[use_knowadge, vector_search_top_k, 
                    history_len, temperature, top_p, msg, chatbot],
            outputs=[msg, chatbot],
            queue=True)
        # åœæ­¢æŒ‰é’®æ§½å‡½æ•°
        stop.click(
            fn=None,
            inputs=None,
            outputs=None,
            cancels=[msg_event, submit_event],
            queue=False)
        # æ¸…ç©ºæŒ‰é’®æ§½å‡½æ•°
        clear.click(
            fn=onButtomClear,
            inputs=None,
            outputs=[state, chatbot],
            queue=False)

    demo.queue(max_size=16,
            concurrency_count=2)
    demo.launch(server_name="0.0.0.0",
                server_port=7860,
                show_error=True,
                share=False, 
                debug=False, 
                enable_queue=True)
    

if __name__ == '__main__':
    main()