import gradio as gr
import random
import time

from util import wenxin_to_chatbot, chatbot_to_wenxin
from api import ACCESS_TIME, ACCESS_TOKEN
from api import update_access_token, chat_by_token


llm_model_dict = {'baidu-wenxin': '',
                  'xunfei-xinghuo': ''}
embedding_model_dict = {"text2vec-base": '',
                        "m3e": ''}


def predict_base(message, chatbot, history:list=[]):
    """æ–‡æœ¬ç”Ÿæˆä¸»å‡½æ•°, ä¸»è¦ç”¨æˆ·è°ƒè¯•æ¯æ¬¡éƒ½è¿”å›å›ºå®šçš„å­—ç¬¦ä¸²
    
    Params
        message: è¾“å…¥
        history: 

    Return
        response: å¯¹åº”äºæ‰€æœ‰ç”¨æˆ·å’Œæœºå™¨äººå“åº”çš„å­—ç¬¦ä¸²å…ƒç»„åˆ—è¡¨ã€‚
                  è¿™å°†åœ¨ Gradio æ¼”ç¤ºä¸­å‘ˆç°ä¸ºè¾“å‡ºã€‚
        history: æ‰€æœ‰ç”¨æˆ·å’Œæœºå™¨äººå“åº”çš„ä»¤ç‰Œè¡¨ç¤ºã€‚
                 åœ¨æœ‰çŠ¶æ€çš„ Gradio æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨å‡½æ•°ç»“æŸæ—¶è¿”å›æ›´æ–°çš„çŠ¶æ€ã€‚
    """
    history.append({"role": "user", "content": message})
    response = "ä½ å¥½ä½ å¥½"
    history.append({"role": "assistant", "content": response})
    
    messages = [(history[i]["content"], history[i+1]["content"]) for i in range(0, len(history)-1, 2)]

    return "", messages, history


def predict_wenxin(message, chatbot):
    """æ–‡æœ¬ç”Ÿæˆä¸»å‡½æ•°, è°ƒç”¨æ–‡å¿ƒä¸€è¨€æ¥å£"""
    access_time, access_token = update_access_token(ACCESS_TIME, ACCESS_TOKEN)

    history = chatbot_to_wenxin(chatbot)
    history.append({"role": "user","content": message})
    reponse = chat_by_token(access_token, history, stream=False)

    # å­—ç¬¦ä¸²ç»“æœ
    result = reponse["result"]
    # tokensæ•°
    prompt_tokens, completion_tokens = reponse["usage"]["prompt_tokens"], reponse["usage"]["completion_tokens"]

    history.append({"role": "assistant","content": result})
    messages = wenxin_to_chatbot(history)

    return "", messages


def on_clear():
    print('å·²æ¸…ç©ºå†å²æ¶ˆæ¯')
    return [], None


with gr.Blocks(title="LLM",
               theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
                ## ğŸš€LLMåŠ©æ‰‹
                ä½¿ç”¨æ–‡å¿ƒä¸€è¨€å•†ç”¨APIå®ç°çš„å¤§è¯­è¨€æ¨¡å‹åŠ©æ‰‹        
                """)
    history = []
    state = gr.State([])

    with gr.Row():
        with gr.Column(scale=1):
            with gr.Accordion("æ¨¡å‹é€‰æ‹©"):
                large_language_model = gr.Dropdown(
                    list(llm_model_dict.keys()),
                    label="large language model",
                    value=list(list(llm_model_dict.keys()))[0])
                embedding_model = gr.Dropdown(
                    list(embedding_model_dict.keys()),
                    label="Embedding model",
                    value=list(embedding_model_dict.keys())[0])
            
            with gr.Accordion("ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶"):
                upload_file = gr.File(
                    label='ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶',
                    file_types=['.txt', '.md', '.docx'])

            with gr.Accordion("æ¨¡å‹å‚æ•°é…ç½®"):
                use_web = gr.Radio(["True", "False"],
                                   label="Web Search",
                                   value="False")
                VECTOR_SEARCH_TOP_K = gr.Slider(
                    1,
                    10,
                    value=6,
                    step=1,
                    label="vector search top k",
                    interactive=True)
                HISTORY_LEN = gr.Slider(
                    0,
                    3,
                    value=0,
                    step=1,
                    label="history len",
                    interactive=True)
                temperature = gr.Slider(
                    0,
                    1,
                    value=0.01,
                    step=0.01,
                    label="temperature",
                    interactive=True)
                top_p = gr.Slider(
                    0,
                    1,
                    value=0.9,
                    step=0.1,
                    label="top_p",
                    interactive=True)

        with gr.Column(scale=4):
            chatbot = gr.Chatbot().style(height=450)
            msg = gr.Textbox(
                label="Chat Message Box",
                placeholder="è¯·è¾“å…¥è¦æé—®çš„é—®é¢˜",
                show_label=False).style(container=False)
            
            gr.Examples(
                examples=["å†™ä¸€ä¸ªå¤§å­¦ç”Ÿå¿ƒç†æƒ…æ™¯å‰§å¤§èµ›çš„å‚èµ›å‰§æœ¬ï¼Œæè¿°å¤§å­¦ç”Ÿæ´»ä¸­ï¼Œå°‘å¹´Aä»è¿·èŒ«åˆ°æ‰¾åˆ°å¥‹æ–—æ–¹å‘çš„æ•…äº‹",
                        "å¸®æˆ‘è®¾è®¡ä¸€ä¸ªå¯ä»¥æ‰“å‡ºsinå‡½æ•°çš„ä»£ç ",
                        "æŸ¥ä¸€ä¸ªçŸ¥è¯†ï¼š"],
                inputs=msg,
                outputs=None,
                cache_examples=False,
                label="å¿«é€Ÿæé—®")

            with gr.Column():
                with gr.Row():
                    submit = gr.Button("Submit")
                    stop = gr.Button("Stop")
                    clear = gr.Button("Clear")
    
    # è¾“å…¥æ¡†å›è½¦æ§½å‡½æ•°
    msg_event = msg.submit(
        fn=predict_wenxin,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot],
        queue=True,
    )
    # æäº¤æŒ‰é’®æ§½å‡½æ•°
    submit_event = submit.click(
        fn=predict_wenxin,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot],
        queue=True,
    )
    # åœæ­¢æŒ‰é’®æ§½å‡½æ•°
    stop.click(
        fn=None,
        inputs=None,
        outputs=None,
        cancels=[msg_event, submit_event],
        queue=False,
    )
    # æ¸…ç©ºæŒ‰é’®æ§½å‡½æ•°
    clear.click(
        fn=on_clear,
        inputs=None,
        outputs=[state, chatbot],
        queue=False,
    )

demo.queue(max_size=16,
           concurrency_count=2)
demo.launch(server_name="0.0.0.0",
            server_port=7860,
            show_error=True,
            share=False, 
            debug=False, 
            enable_queue=True)